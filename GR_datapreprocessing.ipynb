{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GR_datapreprocessing.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J7Lhn60c-sOV",
        "colab_type": "text"
      },
      "source": [
        "## Section 1:  Data Preprocessing for the quantified ingredients "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1o9ucY9upbqQ",
        "colab_type": "code",
        "outputId": "d50157ee-b520-4d66-81cd-09b387aeeb10",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "%cd /gdrive"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n",
            "/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HxCurfCzjPtI",
        "colab_type": "code",
        "outputId": "a9c24c68-2f47-4499-e57e-0f737c44c9bd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "#installing all the libraries needed for the task\n",
        "import pandas as pd\n",
        "import json\n",
        "import nltk\n",
        "from pandas.io.json import json_normalize\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "import re  \n",
        "from nltk.corpus import stopwords\n",
        "stops1 = set(stopwords.words(\"english\"))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7l4t-P18yK8g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#converting json to pandas dataframe\n",
        "with open('/gdrive/My Drive/Colab Notebooks/layer1.json', 'r') as f:\n",
        "    data = json.load(f)\n",
        "    \n",
        "df = pd.DataFrame(data)\n",
        "df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5wDSluzVEPkO",
        "colab_type": "code",
        "outputId": "cdc758b9-aca0-436c-f526-3510517b7bd1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "source": [
        "df.ingredients.head(20)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0     [{'text': '6 ounces penne'}, {'text': '2 cups ...\n",
              "1     [{'text': '1 c. elbow macaroni'}, {'text': '1 ...\n",
              "2     [{'text': '8 tomatoes, quartered'}, {'text': '...\n",
              "3     [{'text': '2 12 cups milk'}, {'text': '1 12 cu...\n",
              "4     [{'text': '1 (3 ounce) package watermelon gela...\n",
              "5     [{'text': '12 cup shredded coconut'}, {'text':...\n",
              "6     [{'text': '2 Chicken thighs'}, {'text': '2 tsp...\n",
              "7     [{'text': '6 -8 cups fresh rhubarb, or'}, {'te...\n",
              "8     [{'text': '8 ounces, weight Light Fat Free Van...\n",
              "9     [{'text': '2 cups flour'}, {'text': '1 tablesp...\n",
              "10    [{'text': '1/2 cup green onions, chopped'}, {'...\n",
              "11    [{'text': '1 teaspoon fennel seeds'}, {'text':...\n",
              "12    [{'text': '1 (750 ml) bottle rose wine, chille...\n",
              "13    [{'text': '14 cup butter'}, {'text': '34 cup b...\n",
              "14    [{'text': '200 grams Cake flour'}, {'text': '1...\n",
              "15    [{'text': '1 can tomato sauce'}, {'text': '1 c...\n",
              "16    [{'text': '1 12 lbs ground beef'}, {'text': '3...\n",
              "17    [{'text': '1 (10 ounce) package frozen chopped...\n",
              "18    [{'text': '1/2 cup A.1. Classic Marinade'}, {'...\n",
              "19    [{'text': '1 cup lentils'}, {'text': '12 onion...\n",
              "Name: ingredients, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nPal-wwDE4KV",
        "colab_type": "code",
        "outputId": "fbf402a0-5f74-4e52-f61f-93b146562eba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "ingredients = pd.io.json.json_normalize(data, record_path='ingredients')\n",
        "ingredients.tail()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>9605389</th>\n",
              "      <td>8 cups water</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9605390</th>\n",
              "      <td>2/3 cup packed light brown sugar</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9605391</th>\n",
              "      <td>1 teaspoon molasses</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9605392</th>\n",
              "      <td>2 (3-inch) cinnamon sticks</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9605393</th>\n",
              "      <td>1 cup medium-grind coffee (not espresso)</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                             text\n",
              "9605389                              8 cups water\n",
              "9605390          2/3 cup packed light brown sugar\n",
              "9605391                       1 teaspoon molasses\n",
              "9605392                2 (3-inch) cinnamon sticks\n",
              "9605393  1 cup medium-grind coffee (not espresso)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jHJkGtUZWbVB",
        "colab_type": "code",
        "outputId": "c8260b45-7434-436d-df5d-3f755c3e6061",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#Counting the number of occurrences of quantities\n",
        "#list = {\"ounce\",\"cup\",\"teaspoon\",\"lbs\",\"tbsp\",\"pound\",\"piece\",\"slice\",\"dozen\",\"gram\",\"pinch\",\"tablespoons\"}\n",
        "# making series \n",
        "#series = pd.Series(list) \n",
        "  \n",
        "ingredients.text.str.count(\"shot\")\n",
        "ingredients.text.str.count(\"shot\").sum()\n",
        "#ingredients.info"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "898"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KI25uipzcA8i",
        "colab_type": "code",
        "outputId": "7b636108-6d37-47fc-8f98-700dae90e641",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 669
        }
      },
      "source": [
        "#Creating a df of the counts and visualising it\n",
        "data = [['ounce', 663540], ['cup', 2671960], ['teaspoon', 1436012],['lbs',72205],['tbsp',133997],['pound',162761],['piece',86804],['slice',363711],['dozen',2396],['gram',53798],['pinch',54814],['tablespoons',703579],['bushel',59],['drop',9403],['dash',41050],['gallon',3690],['glass',1584],['pint',22345],['quart',50416],['scoop',2801],['shot',898]] \n",
        "df_quantity = pd.DataFrame(data, columns = ['Unit', 'Occurences'])\n",
        "df_quantity.head(20)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unit</th>\n",
              "      <th>Occurences</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ounce</td>\n",
              "      <td>663540</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>cup</td>\n",
              "      <td>2671960</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>teaspoon</td>\n",
              "      <td>1436012</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>lbs</td>\n",
              "      <td>72205</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>tbsp</td>\n",
              "      <td>133997</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>pound</td>\n",
              "      <td>162761</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>piece</td>\n",
              "      <td>86804</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>slice</td>\n",
              "      <td>363711</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>dozen</td>\n",
              "      <td>2396</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>gram</td>\n",
              "      <td>53798</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>pinch</td>\n",
              "      <td>54814</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>tablespoons</td>\n",
              "      <td>703579</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>bushel</td>\n",
              "      <td>59</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>drop</td>\n",
              "      <td>9403</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>dash</td>\n",
              "      <td>41050</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>gallon</td>\n",
              "      <td>3690</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>glass</td>\n",
              "      <td>1584</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>pint</td>\n",
              "      <td>22345</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>quart</td>\n",
              "      <td>50416</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>scoop</td>\n",
              "      <td>2801</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           Unit  Occurences\n",
              "0         ounce      663540\n",
              "1           cup     2671960\n",
              "2      teaspoon     1436012\n",
              "3           lbs       72205\n",
              "4          tbsp      133997\n",
              "5         pound      162761\n",
              "6         piece       86804\n",
              "7         slice      363711\n",
              "8         dozen        2396\n",
              "9          gram       53798\n",
              "10        pinch       54814\n",
              "11  tablespoons      703579\n",
              "12       bushel          59\n",
              "13         drop        9403\n",
              "14         dash       41050\n",
              "15       gallon        3690\n",
              "16        glass        1584\n",
              "17         pint       22345\n",
              "18        quart       50416\n",
              "19        scoop        2801"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9qb6SBe1dpIA",
        "colab_type": "code",
        "outputId": "5b62aff8-6277-4b8d-f3af-182c574a9148",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        }
      },
      "source": [
        "#Visualising the number of occurences of the units\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "ax = df_quantity.plot.bar(x='Unit', y='Occurences', rot=0)\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZMAAAEKCAYAAADXdbjqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BWYFf-IIowtT",
        "colab_type": "code",
        "outputId": "cabd04a5-af10-4c8b-8276-c08111440adb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "#Data Cleansing starts\n",
        "train1 = pd.DataFrame(pd.concat([ ingredients['text']], axis=1))\n",
        "train1.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>6 ounces penne</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2 cups Beechers Flagship Cheese Sauce (recipe ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1 ounce Cheddar, grated (1/4 cup)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1 ounce Gruyere cheese, grated (1/4 cup)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1/4 to 1/2 teaspoon chipotle chili powder (see...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text\n",
              "0                                     6 ounces penne\n",
              "1  2 cups Beechers Flagship Cheese Sauce (recipe ...\n",
              "2                  1 ounce Cheddar, grated (1/4 cup)\n",
              "3           1 ounce Gruyere cheese, grated (1/4 cup)\n",
              "4  1/4 to 1/2 teaspoon chipotle chili powder (see..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qvEdkEv4hgMF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train1.to_csv(r'/gdrive/My Drive/Colab Notebooks/d_ingredients_quantity.txt', header=True, index=False, sep='\\t', mode='a')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "maUJtkswnb3H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def clean_sent(sent):\n",
        "    #sent = sent.lower()\n",
        "    sent = re.sub(u'[_\"\\-;%()|+&=*%.,!?:#$@\\[\\]/]',' ',sent)\n",
        "    return sent\n",
        "def clean(df):\n",
        "    df['text'] = df.text.map(lambda x: ' '.join([ word for word in\n",
        "                                                         nltk.word_tokenize(clean_sent(x))]).encode('utf-8'))\n",
        "\n",
        "    #df['instructions'] = df.instructions.map(lambda x: ' '.join([ word for word in\n",
        "                                                         #nltk.word_tokenize(clean_sent(x))]).encode('utf-8'))\n",
        "def removeStopWords(df, stop):\n",
        "\tdf['text'] = df.text.map(lambda x: ' '.join([word for word in nltk.word_tokenize(x.decode('utf-8'))\n",
        "                                                         if word not in stop]).encode('utf-8'))\n",
        "\t#df['instructions'] = df.instructions.map(lambda x: ' '.join([word for word in nltk.word_tokenize(x.decode('utf-8'))\n",
        "                                                         #if word not in stop]).encode('utf-8'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DKtwyh8mnooY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Calling Clean and Stop words on Ingredients df\n",
        "clean(train1)\n",
        "removeStopWords(train1, stops1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c7bnpZrWnsKk",
        "colab_type": "code",
        "outputId": "95476c7a-3ae2-4c2f-b6ed-48a7b79b8595",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "#Calling out the pre-processed data frame named train1\n",
        "train1.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>b'6 ounces penne'</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>b'2 cups Beechers Flagship Cheese Sauce recipe...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>b'1 ounce Cheddar grated 1 4 cup'</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>b'1 ounce Gruyere cheese grated 1 4 cup'</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>b'1 4 1 2 teaspoon chipotle chili powder see N...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text\n",
              "0                                  b'6 ounces penne'\n",
              "1  b'2 cups Beechers Flagship Cheese Sauce recipe...\n",
              "2                  b'1 ounce Cheddar grated 1 4 cup'\n",
              "3           b'1 ounce Gruyere cheese grated 1 4 cup'\n",
              "4  b'1 4 1 2 teaspoon chipotle chili powder see N..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p7_aXALsS3c_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Writing to a CSV\n",
        "#train1.to_csv(r'/gdrive/My Drive/Colab Notebooks/d_ingredients_quantity.csv', index=False, header=True)\n",
        "train1.to_csv(r'/gdrive/My Drive/Colab Notebooks/d_ingredients_quantity.txt', header=True, index=False, sep='\\t', mode='a')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PKkA_BMo_Hzv",
        "colab_type": "text"
      },
      "source": [
        "## Section 2 : Implementation for word embeddings\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TEPwLLPS_Ofw",
        "colab_type": "code",
        "outputId": "4886d0e8-6043-45ac-a9c4-9ad72400929d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1329
        }
      },
      "source": [
        "! pip install gensim\n",
        "! pip install seaborn\n",
        "! pip install elasticsearch\n",
        "! pip install esengine\n",
        "! pip install ipdb"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.6/dist-packages (3.6.0)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.8.4)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.12.0)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.16.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim) (2.21.0)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim) (1.9.162)\n",
            "Requirement already satisfied: boto>=2.32 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim) (2.49.0)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (2019.3.9)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (1.24.3)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim) (0.9.4)\n",
            "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim) (0.2.1)\n",
            "Requirement already satisfied: botocore<1.13.0,>=1.12.162 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim) (1.12.162)\n",
            "Requirement already satisfied: docutils>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.162->boto3->smart-open>=1.2.1->gensim) (0.14)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.162->boto3->smart-open>=1.2.1->gensim) (2.5.3)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.6/dist-packages (0.9.0)\n",
            "Requirement already satisfied: numpy>=1.9.3 in /usr/local/lib/python3.6/dist-packages (from seaborn) (1.16.4)\n",
            "Requirement already satisfied: matplotlib>=1.4.3 in /usr/local/lib/python3.6/dist-packages (from seaborn) (3.0.3)\n",
            "Requirement already satisfied: scipy>=0.14.0 in /usr/local/lib/python3.6/dist-packages (from seaborn) (1.3.0)\n",
            "Requirement already satisfied: pandas>=0.15.2 in /usr/local/lib/python3.6/dist-packages (from seaborn) (0.24.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=1.4.3->seaborn) (0.10.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=1.4.3->seaborn) (2.4.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=1.4.3->seaborn) (2.5.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=1.4.3->seaborn) (1.1.0)\n",
            "Requirement already satisfied: pytz>=2011k in /usr/local/lib/python3.6/dist-packages (from pandas>=0.15.2->seaborn) (2018.9)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from cycler>=0.10->matplotlib>=1.4.3->seaborn) (1.12.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from kiwisolver>=1.0.1->matplotlib>=1.4.3->seaborn) (41.0.1)\n",
            "Collecting elasticsearch\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ae/43/38329621bcca6f0b97e1cc36fb3cef889414a1960fcdc83a41e26b496634/elasticsearch-7.0.2-py2.py3-none-any.whl (83kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 2.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: urllib3>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from elasticsearch) (1.24.3)\n",
            "Installing collected packages: elasticsearch\n",
            "Successfully installed elasticsearch-7.0.2\n",
            "Collecting esengine\n",
            "  Downloading https://files.pythonhosted.org/packages/75/6c/d27dc8c3582ed8ddede6fcec436c5fc0719f36f42cd927cadc20e195c925/esengine-0.0.20-py2.py3-none-any.whl\n",
            "Collecting six==1.10.0 (from esengine)\n",
            "  Downloading https://files.pythonhosted.org/packages/c8/0a/b6723e1bc4c516cb687841499455a8505b44607ab535be01091c0f24f079/six-1.10.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.6/dist-packages (from esengine) (2.5.3)\n",
            "\u001b[31mERROR: s3fs 0.2.1 has requirement six>=1.12.0, but you'll have six 1.10.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: jupyter-console 6.0.0 has requirement prompt-toolkit<2.1.0,>=2.0.0, but you'll have prompt-toolkit 1.0.16 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement six~=1.12.0, but you'll have six 1.10.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Installing collected packages: six, esengine\n",
            "  Found existing installation: six 1.12.0\n",
            "    Uninstalling six-1.12.0:\n",
            "      Successfully uninstalled six-1.12.0\n",
            "Successfully installed esengine-0.0.20 six-1.10.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "six"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Collecting ipdb\n",
            "  Downloading https://files.pythonhosted.org/packages/6d/43/c3c2e866a8803e196d6209595020a4a6db1a3c5d07c01455669497ae23d0/ipdb-0.12.tar.gz\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from ipdb) (41.0.1)\n",
            "Requirement already satisfied: ipython>=5.1.0 in /usr/local/lib/python3.6/dist-packages (from ipdb) (5.5.0)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.6/dist-packages (from ipython>=5.1.0->ipdb) (4.3.2)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from ipython>=5.1.0->ipdb) (4.4.0)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.6/dist-packages (from ipython>=5.1.0->ipdb) (2.1.3)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.6/dist-packages (from ipython>=5.1.0->ipdb) (0.8.1)\n",
            "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from ipython>=5.1.0->ipdb) (4.7.0)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.6/dist-packages (from ipython>=5.1.0->ipdb) (1.0.16)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.6/dist-packages (from ipython>=5.1.0->ipdb) (0.7.5)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.6/dist-packages (from traitlets>=4.2->ipython>=5.1.0->ipdb) (0.2.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from traitlets>=4.2->ipython>=5.1.0->ipdb) (1.10.0)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.6/dist-packages (from pexpect; sys_platform != \"win32\"->ipython>=5.1.0->ipdb) (0.6.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython>=5.1.0->ipdb) (0.1.7)\n",
            "Building wheels for collected packages: ipdb\n",
            "  Building wheel for ipdb (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/59/24/91/695211bd228d40fb22dff0ce3f05ba41ab724ab771736233f3\n",
            "Successfully built ipdb\n",
            "Installing collected packages: ipdb\n",
            "Successfully installed ipdb-0.12\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dSYcMx7t_UD9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import print_function\n",
        "\n",
        "# Handle data\n",
        "import json\n",
        "import operator\n",
        "import collections\n",
        "import re\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from pprint import pprint\n",
        "\n",
        "# Model Algorithms\n",
        "from gensim.models import FastText\n",
        "\n",
        "# Modelling Helpers, see above the description\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "from gensim.models import KeyedVectors\n",
        "# PCA for dimensionality reduction\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Visualisation\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as mpatches\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "import os"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gRS61-f4_aaA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "os.chdir('/gdrive/My Drive/Colab Notebooks/')\n",
        "from preprocess import *\n",
        "from sub_find import *\n",
        "from true_subs import *\n",
        "from constants import *\n",
        "from graph_evaluation import *"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6cXOECEw_ijf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from recipe import Recipe\n",
        "def load_data_from_file(filename=None, ing_file_name=None, local=True, save=False):\n",
        "    return load_data(file_name=filename, ing_file_name=ing_file_name, local=local, save=save)\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_jPJl5h__yIi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "os.chdir('/gdrive/My Drive/Colab Notebooks/')\n",
        "recipes = load_data_from_file(\"layer1.json\", ing_file_name = \"d_ingredients_quantity.txt\",local=True, save=False )\n",
        "clean_recipes = [x.proccessed_ing_list for x in recipes]\n",
        "#recipes = load_data_from_file(\"100ksample2\", \"dbpedia_ingredients.txt\")\n",
        "#sample100k2 = [x.proccessed_ing_list for x in recipes]\n",
        "#clean_recipes.extend(sample100k2) \n",
        "type(clean_recipes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SpdGZQC9_4qU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_features = 300    # Word vector dimensionality                      \n",
        "min_word_count = 3                        \n",
        "num_workers = 4       # Number of CPUs\n",
        "context = 8          # Context window size; \n",
        "                      # let's use avg recipte size                                                                                  \n",
        "downsampling = 1e-3   \n",
        "                    # higher-frequency words are randomly downsampled\n",
        "\n",
        "# Initialize and train the model \n",
        "model = FastText(workers=num_workers, \\\n",
        "            size=num_features,window=context, min_count = min_word_count)\n",
        "model.build_vocab(clean_recipes)\n",
        "model.train(sentences = clean_recipes, total_examples = model.corpus_count, epochs=model.iter)\n",
        "model.save(\"ingredient2vec_fattext.model\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ufyqQEwr_53n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loaded_model = FastText.load('ingredient2vec_fattext.model')\n",
        "print(loaded_model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6RAeVDZH_9xF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loaded_model.similarity('ground beef', 'turkish delight')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "71DoaHyAAA3d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('carrot' in loaded_model.wv.vocab)\n",
        "print('carrots' in loaded_model.wv.vocab)\n",
        "#print(model['carrot'])\n",
        "print(loaded_model['carrots']) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d4aPjQC1ADtB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loaded_model.similarity(\"carrot\", \"carrots\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QI_7FhqkAHtU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loaded_model.most_similar(u'heavy cream')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XflSrOSqAKP6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.manifold import TSNE"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6QFd4OuZAVCO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def display_closestwords_tsnescatterplot(model, word):\n",
        "    \n",
        "    arr = np.empty((0,300), dtype='f') # Since 300 dimensional\n",
        "    word_labels = [word]\n",
        "\n",
        "    # get close words\n",
        "    close_words = model.similar_by_word(word)\n",
        "    \n",
        "    # add the vector for each of the closest words to the array\n",
        "    arr = np.append(arr, np.array([model[word]]), axis=0)\n",
        "    for wrd_score in close_words:\n",
        "        wrd_vector = model[wrd_score[0]]\n",
        "        word_labels.append(wrd_score[0])\n",
        "        arr = np.append(arr, np.array([wrd_vector]), axis=0)\n",
        "        \n",
        "    # find tsne coords for 2 dimensions\n",
        "    tsne = TSNE(n_components=2, random_state=0)\n",
        "    np.set_printoptions(suppress=True)\n",
        "    Y = tsne.fit_transform(arr)\n",
        "\n",
        "    x_coords = Y[:, 0]\n",
        "    y_coords = Y[:, 1]\n",
        "    # display scatter plot\n",
        "    plt.scatter(x_coords, y_coords)\n",
        "\n",
        "    for label, x, y in zip(word_labels, x_coords, y_coords):\n",
        "        plt.annotate(label, xy=(x, y), xytext=(0, 0), textcoords='offset points')\n",
        "    plt.xlim(x_coords.min()+0.00005, x_coords.max()+0.00005)\n",
        "    plt.ylim(y_coords.min()+0.00005, y_coords.max()+0.00005)\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W2O_4RvmAYht",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "display_closestwords_tsnescatterplot(loaded_model, u'carrots')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "StIQPacWAbEw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aglfYJfVAeFL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tsne_plot(model, plt_name='word_embeddings_fast_text.png'):\n",
        "    \"Creates and TSNE model and plots it\"\n",
        "    labels = []\n",
        "    tokens = []\n",
        "\n",
        "    for word in model.wv.vocab:\n",
        "        tokens.append(model[word])\n",
        "        labels.append(word)\n",
        "    \n",
        "    tsne_model = TSNE(perplexity=40, n_components=2, init='pca', n_iter=2500, random_state=23)\n",
        "    new_values = tsne_model.fit_transform(tokens)\n",
        "\n",
        "    x = []\n",
        "    y = []\n",
        "    for value in new_values:\n",
        "        x.append(value[0])\n",
        "        y.append(value[1])\n",
        "        \n",
        "    plt.figure(figsize=(16, 16)) \n",
        "    for i in range(len(x)):\n",
        "        plt.scatter(x[i],y[i])\n",
        "        plt.annotate(labels[i],\n",
        "                     xy=(x[i], y[i]),\n",
        "                     xytext=(5, 2),\n",
        "                     textcoords='offset points',\n",
        "                     ha='right',\n",
        "                     va='bottom')\n",
        "    plt.savefig(plt_name, dpi = 300)\n",
        "    plt.show()\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EW4bAeO6Ag8u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tsne_plot(loaded_model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YA7nH137SK_0",
        "colab_type": "text"
      },
      "source": [
        "##Section 3: Gensim Word2Vec Implementation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PaUF6enVSQsb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1278
        },
        "outputId": "3824e4f3-1e35-428f-e7dc-74073cb662df"
      },
      "source": [
        "!pip3 install -U gensim\n",
        "!pip install tensorflow==1.2.0 --ignore-installed\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting gensim\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d3/4b/19eecdf07d614665fa889857dc56ac965631c7bd816c3476d2f0cac6ea3b/gensim-3.7.3-cp36-cp36m-manylinux1_x86_64.whl (24.2MB)\n",
            "\u001b[K     |████████████████████████████████| 24.2MB 3.5MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: smart-open>=1.7.0 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.8.4)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.10.0)\n",
            "Requirement already satisfied, skipping upgrade: scipy>=0.18.1 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.3.0)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.16.4)\n",
            "Requirement already satisfied, skipping upgrade: requests in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.7.0->gensim) (2.21.0)\n",
            "Requirement already satisfied, skipping upgrade: boto>=2.32 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.7.0->gensim) (2.49.0)\n",
            "Requirement already satisfied, skipping upgrade: boto3 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.7.0->gensim) (1.9.162)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.7.0->gensim) (2019.3.9)\n",
            "Requirement already satisfied, skipping upgrade: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.7.0->gensim) (2.8)\n",
            "Requirement already satisfied, skipping upgrade: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.7.0->gensim) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.7.0->gensim) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.7.0->gensim) (0.9.4)\n",
            "Requirement already satisfied, skipping upgrade: botocore<1.13.0,>=1.12.162 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.7.0->gensim) (1.12.162)\n",
            "Requirement already satisfied, skipping upgrade: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.7.0->gensim) (0.2.1)\n",
            "Requirement already satisfied, skipping upgrade: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.162->boto3->smart-open>=1.7.0->gensim) (2.5.3)\n",
            "Requirement already satisfied, skipping upgrade: docutils>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.162->boto3->smart-open>=1.7.0->gensim) (0.14)\n",
            "Installing collected packages: gensim\n",
            "  Found existing installation: gensim 3.6.0\n",
            "    Uninstalling gensim-3.6.0:\n",
            "      Successfully uninstalled gensim-3.6.0\n",
            "Successfully installed gensim-3.7.3\n",
            "Collecting tensorflow==1.2.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5e/55/7995cc1e9e60fa37ea90e6777d832e75026fde5c6109215d892aaff2e9b7/tensorflow-1.2.0-cp36-cp36m-manylinux1_x86_64.whl (35.0MB)\n",
            "\u001b[K     |████████████████████████████████| 35.0MB 63.0MB/s \n",
            "\u001b[?25hCollecting html5lib==0.9999999 (from tensorflow==1.2.0)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ae/ae/bcb60402c60932b32dfaf19bb53870b29eda2cd17551ba5639219fb5ebf9/html5lib-0.9999999.tar.gz (889kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 23.1MB/s \n",
            "\u001b[?25hCollecting backports.weakref==1.0rc1 (from tensorflow==1.2.0)\n",
            "  Downloading https://files.pythonhosted.org/packages/6a/f7/ae34b6818b603e264f26fe7db2bd07850ce331ce2fde74b266d61f4a2d87/backports.weakref-1.0rc1-py3-none-any.whl\n",
            "Collecting werkzeug>=0.11.10 (from tensorflow==1.2.0)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9f/57/92a497e38161ce40606c27a86759c6b92dd34fcdb33f64171ec559257c02/Werkzeug-0.15.4-py2.py3-none-any.whl (327kB)\n",
            "\u001b[K     |████████████████████████████████| 327kB 33.0MB/s \n",
            "\u001b[?25hCollecting bleach==1.5.0 (from tensorflow==1.2.0)\n",
            "  Downloading https://files.pythonhosted.org/packages/33/70/86c5fec937ea4964184d4d6c4f0b9551564f821e1c3575907639036d9b90/bleach-1.5.0-py2.py3-none-any.whl\n",
            "Collecting six>=1.10.0 (from tensorflow==1.2.0)\n",
            "  Downloading https://files.pythonhosted.org/packages/73/fb/00a976f728d0d1fecfe898238ce23f502a721c0ac0ecfedb80e0d88c64e9/six-1.12.0-py2.py3-none-any.whl\n",
            "Collecting wheel>=0.26 (from tensorflow==1.2.0)\n",
            "  Downloading https://files.pythonhosted.org/packages/bb/10/44230dd6bf3563b8f227dbf344c908d412ad2ff48066476672f3a72e174e/wheel-0.33.4-py2.py3-none-any.whl\n",
            "Collecting markdown==2.2.0 (from tensorflow==1.2.0)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ac/99/288a81a38526a42c98b5b9832c6e339ca8d5dd38b19a53abfac7c8037c7f/Markdown-2.2.0.tar.gz (236kB)\n",
            "\u001b[K     |████████████████████████████████| 245kB 33.5MB/s \n",
            "\u001b[?25hCollecting numpy>=1.11.0 (from tensorflow==1.2.0)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/87/2d/e4656149cbadd3a8a0369fcd1a9c7d61cc7b87b3903b85389c70c989a696/numpy-1.16.4-cp36-cp36m-manylinux1_x86_64.whl (17.3MB)\n",
            "\u001b[K     |████████████████████████████████| 17.3MB 1.2MB/s \n",
            "\u001b[?25hCollecting protobuf>=3.2.0 (from tensorflow==1.2.0)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d2/fb/29de8d08967f0cce1bb10b39846d836b0f3bf6776ddc36aed7c73498ca7e/protobuf-3.8.0-cp36-cp36m-manylinux1_x86_64.whl (1.2MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2MB 28.2MB/s \n",
            "\u001b[?25hCollecting setuptools (from protobuf>=3.2.0->tensorflow==1.2.0)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ec/51/f45cea425fd5cb0b0380f5b0f048ebc1da5b417e48d304838c02d6288a1e/setuptools-41.0.1-py2.py3-none-any.whl (575kB)\n",
            "\u001b[K     |████████████████████████████████| 583kB 31.2MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: html5lib, markdown\n",
            "  Building wheel for html5lib (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/50/ae/f9/d2b189788efcf61d1ee0e36045476735c838898eef1cad6e29\n",
            "  Building wheel for markdown (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/b6/52/17/f0af18e3e0ec6fa60b361ffed15b4c3468f6f3bcdb87fbe079\n",
            "Successfully built html5lib markdown\n",
            "\u001b[31mERROR: tensorboard 1.13.1 has requirement markdown>=2.6.8, but you'll have markdown 2.2.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: stable-baselines 2.2.1 has requirement tensorflow>=1.5.0, but you'll have tensorflow 1.2.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: magenta 0.3.19 has requirement tensorflow>=1.12.0, but you'll have tensorflow 1.2.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: jupyter-console 6.0.0 has requirement prompt-toolkit<2.1.0,>=2.0.0, but you'll have prompt-toolkit 1.0.16 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: esengine 0.0.20 has requirement six==1.10.0, but you'll have six 1.12.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Installing collected packages: six, html5lib, backports.weakref, werkzeug, bleach, wheel, markdown, numpy, setuptools, protobuf, tensorflow\n",
            "Successfully installed backports.weakref-1.0.post1 bleach-3.1.0 html5lib-1.0.1 markdown-3.1.1 numpy-1.16.4 protobuf-3.8.0 setuptools-41.0.1 six-1.12.0 tensorflow-1.13.1 werkzeug-0.15.4 wheel-0.33.4\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google",
                  "numpy",
                  "pkg_resources",
                  "six"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6rqNhZvBSTOQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "aeea81ed-12cb-45ec-8474-509d2ab327bf"
      },
      "source": [
        "import gensim, logging, os, re, string, tensorflow\n",
        "import tensorflow as tf\n",
        "from tensorflow.contrib.tensorboard.plugins import projector\n",
        "from gensim.utils import simple_preprocess\n",
        "import os\n",
        "\n",
        "print('gensim version: \\t%s'     % gensim.__version__)\n",
        "print('TensorFlow version: \\t%s' % tensorflow.__version__)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "gensim version: \t3.7.3\n",
            "TensorFlow version: \t1.2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_I9xFHkRSWyh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# For displaying gensim logs\n",
        "logging.basicConfig(format='%(levelname)s : %(message)s', level=logging.INFO)\n",
        "os.chdir('/gdrive/My Drive/Data')\n",
        "# Directory with raw txt-files\n",
        "TEXT_DIR  = '/gdrive/My Drive/Data'\n",
        "\n",
        "# Directory for saving checkpoint and metadata\n",
        "MODEL_DIR = '/gdrive/My Drive/Dataa'\n",
        "\n",
        "# Word2vec\n",
        "EMBEDDING_SIZE = 300"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XGCK35rNSqsw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "66893477-6811-4b2f-ccee-dcf206128d60"
      },
      "source": [
        "def read_files(path):\n",
        "    \"\"\"\n",
        "    Read in text files\n",
        "    \"\"\"\n",
        "    documents = list()\n",
        "    tokenize  = lambda x: simple_preprocess(x)\n",
        "    \n",
        "    # Read in all files in directory\n",
        "    if os.path.isdir(path):\n",
        "        for filename in os.listdir(path):\n",
        "            with open('%s/%s' % (path, filename), encoding='utf-8') as f:\n",
        "                doc = f.read()\n",
        "                #doc = clean_doc(doc)\n",
        "                documents.append(tokenize(doc))\n",
        "    return documents\n",
        "\n",
        "docs = read_files(TEXT_DIR)\n",
        "print('Number of documents: %i' % len(docs))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of documents: 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IZZkPB9MSu19",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 595
        },
        "outputId": "c67ecdad-a889-436b-dd43-dad4016e15cf"
      },
      "source": [
        "model = gensim.models.Word2Vec(docs, size=EMBEDDING_SIZE)\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO : collecting all words and their counts\n",
            "INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
            "INFO : collected 49282 word types from a corpus of 107817476 raw words and 1 sentences\n",
            "INFO : Loading a fresh vocabulary\n",
            "INFO : effective_min_count=5 retains 26341 unique words (53% of original 49282, drops 22941)\n",
            "INFO : effective_min_count=5 leaves 107748733 word corpus (99% of original 107817476, drops 68743)\n",
            "INFO : deleting the raw counts dictionary of 49282 items\n",
            "INFO : sample=0.001 downsamples 76 most-common words\n",
            "INFO : downsampling leaves estimated 76827634 word corpus (71.3% of prior 107748733)\n",
            "INFO : estimated required memory for 26341 words and 300 dimensions: 76388900 bytes\n",
            "INFO : resetting layer weights\n",
            "INFO : training model with 3 workers on 26341 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
            "INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "INFO : EPOCH - 1 : training on 107817476 raw words (10000 effective words) took 0.1s, 181373 effective words/s\n",
            "INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "INFO : EPOCH - 2 : training on 107817476 raw words (10000 effective words) took 0.1s, 187063 effective words/s\n",
            "INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "INFO : EPOCH - 3 : training on 107817476 raw words (10000 effective words) took 0.1s, 188604 effective words/s\n",
            "INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "INFO : EPOCH - 4 : training on 107817476 raw words (10000 effective words) took 0.1s, 185172 effective words/s\n",
            "INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "INFO : EPOCH - 5 : training on 107817476 raw words (10000 effective words) took 0.1s, 171602 effective words/s\n",
            "INFO : training on a 539087380 raw words (50000 effective words) took 0.3s, 154782 effective words/s\n",
            "WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7bcpEvB4Svy9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "fe925d11-ab3a-491d-f2d8-819d7f7578e3"
      },
      "source": [
        "if not os.path.exists(MODEL_DIR):\n",
        "    os.makedirs(MODEL_DIR)\n",
        "model.save(os.path.join(MODEL_DIR,'word2vec'))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO : saving Word2Vec object under /gdrive/My Drive/Dataa/word2vec, separately None\n",
            "INFO : not storing attribute vectors_norm\n",
            "INFO : not storing attribute cum_table\n",
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n",
            "INFO : saved /gdrive/My Drive/Dataa/word2vec\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cZ0mFWTFSyFx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "7ee88858-e607-4b70-d3c3-fdca0354cad2"
      },
      "source": [
        "weights     = model.wv.vectors\n",
        "index_words = model.wv.index2word\n",
        "\n",
        "vocab_size    = weights.shape[0]\n",
        "embedding_dim = weights.shape[1]\n",
        "\n",
        "print('Shape of weights:', weights.shape)\n",
        "print('Vocabulary size: %i' % vocab_size)\n",
        "print('Embedding size: %i'  % embedding_dim)\n",
        "\n",
        "with open(os.path.join(MODEL_DIR,'metadata.tsv'), 'w') as f:\n",
        "    f.writelines(\"\\n\".join(index_words))\n",
        "\n",
        "# Required if you re-run without restarting the kernel\n",
        "tf.reset_default_graph()\n",
        "    \n",
        "W = tf.Variable(tf.constant(0.0, shape=[vocab_size, embedding_dim]), trainable=False, name=\"W\")\n",
        "embedding_placeholder = tf.placeholder(tf.float32, [vocab_size, embedding_dim])\n",
        "\n",
        "embedding_init = W.assign(embedding_placeholder)\n",
        "writer = tf.summary.FileWriter(MODEL_DIR, graph=tf.get_default_graph())\n",
        "saver = tf.train.Saver()\n",
        "\n",
        "config = projector.ProjectorConfig()\n",
        "embedding = config.embeddings.add()\n",
        "embedding.tensor_name = W.name\n",
        "embedding.metadata_path = './metadata.tsv'\n",
        "projector.visualize_embeddings(writer, config)\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    sess.run(embedding_init, feed_dict={embedding_placeholder: weights})\n",
        "    save_path = saver.save(sess, os.path.join(MODEL_DIR, \"model.cpkt\"))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of weights: (26341, 300)\n",
            "Vocabulary size: 26341\n",
            "Embedding size: 300\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tc8PeTcnS0_J",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "eade8119-8d00-46f6-b9fa-9196eac9f810"
      },
      "source": [
        "model.wv.most_similar(positive=['coffee'], topn=10)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO : precomputing L2-norms of word weight vectors\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('roasted', 0.9885692596435547),\n",
              " ('dijon', 0.988249659538269),\n",
              " ('freshly', 0.9882488250732422),\n",
              " ('temperature', 0.9882112741470337),\n",
              " ('skinless', 0.9882060885429382),\n",
              " ('pork', 0.98819899559021),\n",
              " ('halved', 0.988187313079834),\n",
              " ('pinch', 0.9881695508956909),\n",
              " ('black', 0.9881690144538879),\n",
              " ('beef', 0.9881572723388672)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eq2q-AAxUGTD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}